{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:49:47.553113Z",
     "start_time": "2025-08-03T07:49:46.590209Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9822219624976895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:49:55.263758Z",
     "start_time": "2025-08-03T07:49:55.260616Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data(file_path: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    df = pd.read_excel(file_path)\n",
    "    if \"Unnamed: 0\" in df.columns:\n",
    "        df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "    median_ic50 = df[\"IC50, mM\"].median()\n",
    "    y = (df[\"IC50, mM\"] > median_ic50).astype(int)\n",
    "    feature_cols = [c for c in df.columns if c not in [\"IC50, mM\", \"CC50, mM\", \"SI\"]]\n",
    "    X = df[feature_cols]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c9dde026a0dd1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:50:05.106089Z",
     "start_time": "2025-08-03T07:50:05.101887Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_models(random_state: int = 42) -> Dict[str, Tuple[Pipeline, Dict[str, List]]]:\n",
    "    models: Dict[str, Tuple[Pipeline, Dict[str, List]]] = {}\n",
    "    # Logistic Regression\n",
    "    log_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LogisticRegression(max_iter=500, random_state=random_state))\n",
    "    ])\n",
    "    log_grid = {\n",
    "        'model__C': [0.1, 1.0, 10.0],\n",
    "        'model__class_weight': [None, 'balanced']\n",
    "    }\n",
    "    models['LogisticRegression'] = (log_pipe, log_grid)\n",
    "    # Random Forest\n",
    "    rf_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('model', RandomForestClassifier(random_state=random_state))\n",
    "    ])\n",
    "    rf_grid = {\n",
    "        'model__n_estimators': [200, 400],\n",
    "        'model__max_depth': [None, 10],\n",
    "        'model__min_samples_split': [2],\n",
    "        'model__class_weight': [None, 'balanced']\n",
    "    }\n",
    "    models['RandomForest'] = (rf_pipe, rf_grid)\n",
    "    # Gradient Boosting\n",
    "    gb_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('model', GradientBoostingClassifier(random_state=random_state))\n",
    "    ])\n",
    "    gb_grid = {\n",
    "        'model__n_estimators': [200],\n",
    "        'model__learning_rate': [0.05, 0.1],\n",
    "        'model__max_depth': [3, 5]\n",
    "    }\n",
    "    models['GradientBoosting'] = (gb_pipe, gb_grid)\n",
    "    # XGBoost\n",
    "    xgb_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('model', XGBClassifier(\n",
    "            random_state=random_state,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            tree_method='hist',\n",
    "            n_jobs=4\n",
    "        ))\n",
    "    ])\n",
    "    xgb_grid = {\n",
    "        'model__n_estimators': [400],\n",
    "        'model__max_depth': [3, 6],\n",
    "        'model__learning_rate': [0.05, 0.1],\n",
    "        'model__subsample': [0.8],\n",
    "        'model__scale_pos_weight': [1]\n",
    "    }\n",
    "    models['XGBoost'] = (xgb_pipe, xgb_grid)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb5b1cc9335fe381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:50:19.979511Z",
     "start_time": "2025-08-03T07:50:19.974948Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_models(models: Dict[str, Tuple[Pipeline, Dict[str, List]]], X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "    scoring = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall',\n",
    "        'f1': 'f1',\n",
    "        'roc_auc': 'roc_auc'\n",
    "    }\n",
    "    results = []\n",
    "    for name, (pipe, grid) in models.items():\n",
    "        print(f\"\\n--- Optimising {name} ---\")\n",
    "        gcv = GridSearchCV(\n",
    "            pipe,\n",
    "            grid,\n",
    "            cv=5,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            error_score='raise'\n",
    "        )\n",
    "        gcv.fit(X, y)\n",
    "        print(f\"Best parameters for {name}: {gcv.best_params_}\")\n",
    "        best = gcv.best_estimator_\n",
    "        cv_scores = cross_validate(best, X, y, cv=5, scoring=scoring, n_jobs=-1)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy (mean)': cv_scores['test_accuracy'].mean(),\n",
    "            'Accuracy (std)': cv_scores['test_accuracy'].std(),\n",
    "            'Precision (mean)': cv_scores['test_precision'].mean(),\n",
    "            'Precision (std)': cv_scores['test_precision'].std(),\n",
    "            'Recall (mean)': cv_scores['test_recall'].mean(),\n",
    "            'Recall (std)': cv_scores['test_recall'].std(),\n",
    "            'F1 (mean)': cv_scores['test_f1'].mean(),\n",
    "            'F1 (std)': cv_scores['test_f1'].std(),\n",
    "            'ROC_AUC (mean)': cv_scores['test_roc_auc'].mean(),\n",
    "            'ROC_AUC (std)': cv_scores['test_roc_auc'].std()\n",
    "        })\n",
    "    results_df = pd.DataFrame(results).sort_values(by='ROC_AUC (mean)', ascending=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "803266b38eec77d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:51:07.131643Z",
     "start_time": "2025-08-03T07:50:35.960697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optimising LogisticRegression ---\n",
      "Best parameters for LogisticRegression: {'model__C': 0.1, 'model__class_weight': None}\n",
      "\n",
      "--- Optimising RandomForest ---\n",
      "Best parameters for RandomForest: {'model__class_weight': 'balanced', 'model__max_depth': None, 'model__min_samples_split': 2, 'model__n_estimators': 400}\n",
      "\n",
      "--- Optimising GradientBoosting ---\n",
      "Best parameters for GradientBoosting: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 200}\n",
      "\n",
      "--- Optimising XGBoost ---\n",
      "Best parameters for XGBoost: {'model__learning_rate': 0.05, 'model__max_depth': 3, 'model__n_estimators': 400, 'model__scale_pos_weight': 1, 'model__subsample': 0.8}\n",
      "\n",
      "===== Cross‑validated performance summary =====\n",
      "             Model  Accuracy (mean)  Accuracy (std)  Precision (mean)  Precision (std)  Recall (mean)  Recall (std)  F1 (mean)  F1 (std)  ROC_AUC (mean)  ROC_AUC (std)\n",
      "  GradientBoosting           0.5833          0.0674            0.5913           0.0835         0.5920        0.1458     0.5806    0.0819          0.5778         0.1036\n",
      "           XGBoost           0.5793          0.0753            0.5783           0.0774         0.5780        0.1847     0.5662    0.1135          0.5634         0.1032\n",
      "      RandomForest           0.5694          0.0478            0.5629           0.0443         0.6060        0.1867     0.5716    0.0947          0.5541         0.0899\n",
      "LogisticRegression           0.5333          0.0980            0.5568           0.1158         0.4820        0.0479     0.5118    0.0672          0.5232         0.1284\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('data.xlsx')\n",
    "X, y = load_and_prepare_data(data_path)\n",
    "models = build_models()\n",
    "results_df = evaluate_models(models, X, y)\n",
    "print(\"\\n===== Cross‑validated performance summary =====\")\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(results_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "results_df.to_csv('classification_ic50_median_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
