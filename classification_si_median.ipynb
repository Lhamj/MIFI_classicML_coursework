{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:54:14.597572Z",
     "start_time": "2025-08-03T07:54:13.398204Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9358335edbdce75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:54:20.843362Z",
     "start_time": "2025-08-03T07:54:20.839936Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data(file_path: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    df = pd.read_excel(file_path)\n",
    "    if \"Unnamed: 0\" in df.columns:\n",
    "        df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "    median_si = df[\"SI\"].median()\n",
    "    y = (df[\"SI\"] > median_si).astype(int)\n",
    "    feature_cols = [c for c in df.columns if c not in [\"IC50, mM\", \"CC50, mM\", \"SI\"]]\n",
    "    X = df[feature_cols]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b2edb3c4f9abbb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:54:32.790190Z",
     "start_time": "2025-08-03T07:54:32.785272Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_models(random_state: int = 42) -> Dict[str, Tuple[Pipeline, Dict[str, List]]]:\n",
    "    models: Dict[str, Tuple[Pipeline, Dict[str, List]]] = {}\n",
    "    # Logistic regression\n",
    "    log_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LogisticRegression(max_iter=500, random_state=random_state))\n",
    "    ])\n",
    "    log_grid = {\n",
    "        'model__C': [0.1, 1.0, 10.0],\n",
    "        'model__class_weight': [None, 'balanced']\n",
    "    }\n",
    "    models['LogisticRegression'] = (log_pipe, log_grid)\n",
    "    # Random Forest\n",
    "    rf_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('model', RandomForestClassifier(random_state=random_state))\n",
    "    ])\n",
    "    rf_grid = {\n",
    "        'model__n_estimators': [200, 400],\n",
    "        'model__max_depth': [None, 10],\n",
    "        'model__min_samples_split': [2],\n",
    "        'model__class_weight': [None, 'balanced']\n",
    "    }\n",
    "    models['RandomForest'] = (rf_pipe, rf_grid)\n",
    "    # Gradient Boosting\n",
    "    gb_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('model', GradientBoostingClassifier(random_state=random_state))\n",
    "    ])\n",
    "    gb_grid = {\n",
    "        'model__n_estimators': [200],\n",
    "        'model__learning_rate': [0.05, 0.1],\n",
    "        'model__max_depth': [3, 5]\n",
    "    }\n",
    "    models['GradientBoosting'] = (gb_pipe, gb_grid)\n",
    "    # XGBoost classifier\n",
    "    xgb_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('model', XGBClassifier(\n",
    "            random_state=random_state,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            tree_method='hist',\n",
    "            n_jobs=4\n",
    "        ))\n",
    "    ])\n",
    "    xgb_grid = {\n",
    "        'model__n_estimators': [400],\n",
    "        'model__max_depth': [3, 6],\n",
    "        'model__learning_rate': [0.05, 0.1],\n",
    "        'model__subsample': [0.8],\n",
    "        'model__scale_pos_weight': [1]\n",
    "    }\n",
    "    models['XGBoost'] = (xgb_pipe, xgb_grid)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5174ac7fb3d19b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:54:46.899636Z",
     "start_time": "2025-08-03T07:54:46.895225Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_models(models: Dict[str, Tuple[Pipeline, Dict[str, List]]], X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "    scoring = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall',\n",
    "        'f1': 'f1',\n",
    "        'roc_auc': 'roc_auc'\n",
    "    }\n",
    "    results = []\n",
    "    for name, (pipe, grid) in models.items():\n",
    "        print(f\"\\n--- Optimising {name} ---\")\n",
    "        gcv = GridSearchCV(\n",
    "            pipe,\n",
    "            grid,\n",
    "            cv=5,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            error_score='raise'\n",
    "        )\n",
    "        gcv.fit(X, y)\n",
    "        print(f\"Best parameters for {name}: {gcv.best_params_}\")\n",
    "        best = gcv.best_estimator_\n",
    "        cv_scores = cross_validate(best, X, y, cv=5, scoring=scoring, n_jobs=-1)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy (mean)': cv_scores['test_accuracy'].mean(),\n",
    "            'Accuracy (std)': cv_scores['test_accuracy'].std(),\n",
    "            'Precision (mean)': cv_scores['test_precision'].mean(),\n",
    "            'Precision (std)': cv_scores['test_precision'].std(),\n",
    "            'Recall (mean)': cv_scores['test_recall'].mean(),\n",
    "            'Recall (std)': cv_scores['test_recall'].std(),\n",
    "            'F1 (mean)': cv_scores['test_f1'].mean(),\n",
    "            'F1 (std)': cv_scores['test_f1'].std(),\n",
    "            'ROC_AUC (mean)': cv_scores['test_roc_auc'].mean(),\n",
    "            'ROC_AUC (std)': cv_scores['test_roc_auc'].std()\n",
    "        })\n",
    "    results_df = pd.DataFrame(results).sort_values(by='ROC_AUC (mean)', ascending=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2187840eb1c0abc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:55:52.063719Z",
     "start_time": "2025-08-03T07:55:15.145148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optimising LogisticRegression ---\n",
      "Best parameters for LogisticRegression: {'model__C': 0.1, 'model__class_weight': None}\n",
      "\n",
      "--- Optimising RandomForest ---\n",
      "Best parameters for RandomForest: {'model__class_weight': 'balanced', 'model__max_depth': 10, 'model__min_samples_split': 2, 'model__n_estimators': 200}\n",
      "\n",
      "--- Optimising GradientBoosting ---\n",
      "Best parameters for GradientBoosting: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 200}\n",
      "\n",
      "--- Optimising XGBoost ---\n",
      "Best parameters for XGBoost: {'model__learning_rate': 0.05, 'model__max_depth': 6, 'model__n_estimators': 400, 'model__scale_pos_weight': 1, 'model__subsample': 0.8}\n",
      "\n",
      "===== Cross‑validated performance summary =====\n",
      "             Model  Accuracy (mean)  Accuracy (std)  Precision (mean)  Precision (std)  Recall (mean)  Recall (std)  F1 (mean)  F1 (std)  ROC_AUC (mean)  ROC_AUC (std)\n",
      "      RandomForest           0.5234          0.0641            0.5464           0.1060         0.5140        0.1900     0.5013    0.1076          0.5300         0.0718\n",
      "           XGBoost           0.5024          0.0592            0.5083           0.0654         0.4940        0.1794     0.4825    0.1043          0.5096         0.0598\n",
      "  GradientBoosting           0.5084          0.0417            0.5157           0.0568         0.4960        0.1488     0.4915    0.0784          0.5080         0.0533\n",
      "LogisticRegression           0.4923          0.1158            0.4805           0.1005         0.5340        0.1892     0.5026    0.1380          0.4922         0.1315\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('data.xlsx')\n",
    "X, y = load_and_prepare_data(data_path)\n",
    "models = build_models()\n",
    "results_df = evaluate_models(models, X, y)\n",
    "print(\"\\n===== Cross‑validated performance summary =====\")\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(results_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "results_df.to_csv('classification_si_median_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
